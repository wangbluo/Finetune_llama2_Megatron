# Finetune_llama2_Megatron

Building a llama fine-tuning script from scratch using PyTorch and the transformers API, with support for four optional parameters: gradient checkpoint, mixed precision, data parallelism, and tensor parallelism. Avoid using ColossalAI/Megatron/DeepSpeed. Referring to existing code is allowed.

The loss curve:
![img_v3_0284_f9313a8e-9e61-41fc-b35f-5d2e7e991aag](https://github.com/wangbluo/Finetune_llama2_Megatron/assets/32676639/14245e6c-8b3a-43c4-93d4-356951606a95)
